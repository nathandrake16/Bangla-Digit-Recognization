{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTbqiN1lKPcU",
        "outputId": "ad1422e0-d952-4472-da62-0eef99b42bfb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1osw_zTGj-lr"
      },
      "source": [
        "# **An Efficient Deep Learning Model for Bangla Handwritten Digit Recognition**\n",
        "\n",
        "\n",
        "> Dataset [**BHand**] : https://github.com/SaadatChowdhury/BHaND\n",
        "\n",
        "*README*\n",
        "\n",
        "The full form of BHaND is \"Bengali Handwritten Numerals Dataset\".\n",
        "\n",
        "In this work, we have created a new dataset containing images of handwritten Bengali numerals. Our main goal was to make the dataset similar to MNIST, one of the most well-known datasets for English handwritten digits.\n",
        "\n",
        "Similar to MNIST, we have collected 70,000 sample images of Bengali digits. Each image is 32Ã—32 in size (MNIST images were 28Ã—28). Everything else closely follows the structure of the MNIST dataset.\n",
        "\n",
        "**Basic Description**\n",
        "\n",
        "This dataset consists of **70,000** samples divided into three sets:\n",
        "\n",
        "> **Training set:** 50,000 samples\n",
        "\n",
        "> **Validation set:** 10,000 samples\n",
        "\n",
        "> **Test set:** 10,000 samples\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Each row in these sets has two components: the first component is the image data, and the second component is the label. The label represents the Bengali digit (0 to 9) shown in the corresponding image. For example, the training set contains 50,000 rows, each with two parts: a 1024-dimensional image vector and an integer label.\n",
        "\n",
        "All samples are in grayscale and normalized to the intensity range\n",
        "[0,1]. The pixel intensities were inverted so that black pixels have a value of 1 and white pixels have a value of 0. The images were then flattened into 1024-dimensional vectors (since\n",
        "32Ã—32=1024).\n",
        "\n",
        "Finally, all samples were grouped together into a single pickle file using Python's `cPickle` serializer and compressed using the GZIP compression algorithm for faster transmission.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QAyREzP-iJ__"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, callbacks, optimizers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import cv2\n",
        "import os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import time\n",
        "import warnings\n",
        "import gzip\n",
        "import pickle\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "class BanglaDigitRecognizer:\n",
        "\n",
        "    def __init__(self, input_shape=(32, 32, 1), num_classes=10):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "\n",
        "    def create_efficient_model(self):\n",
        "        \"\"\"\n",
        "        Create an efficient CNN architecture optimized for Bangla digits (32x32)\n",
        "        Based on research findings with custom optimizations\n",
        "        \"\"\"\n",
        "        model = keras.Sequential([\n",
        "            # Input layer\n",
        "            layers.Input(shape=self.input_shape),\n",
        "\n",
        "            # First convolutional block\n",
        "            layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "            layers.MaxPooling2D((2, 2)),\n",
        "            layers.Dropout(0.25),\n",
        "\n",
        "            # Second convolutional block\n",
        "            layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "            layers.MaxPooling2D((2, 2)),\n",
        "            layers.Dropout(0.25),\n",
        "\n",
        "            # Third convolutional block\n",
        "            layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "            layers.MaxPooling2D((2, 2)),\n",
        "            layers.Dropout(0.25),\n",
        "\n",
        "            # Fourth convolutional block (added for 32x32 input)\n",
        "            layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "            layers.MaxPooling2D((2, 2)),\n",
        "            layers.Dropout(0.25),\n",
        "\n",
        "            # Global Average Pooling (more efficient than Flatten)\n",
        "            layers.GlobalAveragePooling2D(),\n",
        "\n",
        "            # Dense layers with regularization\n",
        "            layers.Dense(512, activation='relu'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Dropout(0.5),\n",
        "\n",
        "            layers.Dense(256, activation='relu'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.Dropout(0.5),\n",
        "\n",
        "            # Output layer\n",
        "            layers.Dense(self.num_classes, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        # Compile with optimized settings\n",
        "        model.compile(\n",
        "            optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        self.model = model\n",
        "        return model\n",
        "\n",
        "    def create_data_generators(self, train_data, train_labels, validation_split=0.2):\n",
        "        \"\"\"\n",
        "        Create data generators with augmentation for training\n",
        "        \"\"\"\n",
        "        # Data augmentation for training\n",
        "        train_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "            rotation_range=10,\n",
        "            width_shift_range=0.1,\n",
        "            height_shift_range=0.1,\n",
        "            shear_range=0.1,\n",
        "            zoom_range=0.1,\n",
        "            fill_mode='constant',\n",
        "            cval=0,\n",
        "            validation_split=validation_split\n",
        "        )\n",
        "\n",
        "        # No augmentation for validation\n",
        "        val_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
        "            validation_split=validation_split\n",
        "        )\n",
        "\n",
        "        train_generator = train_datagen.flow(\n",
        "            train_data, train_labels,\n",
        "            batch_size=32,\n",
        "            subset='training'\n",
        "        )\n",
        "\n",
        "        val_generator = val_datagen.flow(\n",
        "            train_data, train_labels,\n",
        "            batch_size=32,\n",
        "            subset='validation'\n",
        "        )\n",
        "\n",
        "        return train_generator, val_generator\n",
        "\n",
        "    def get_callbacks(self, model_path='best_bangla_model.h5'):\n",
        "        \"\"\"\n",
        "        Create training callbacks for optimal performance\n",
        "        \"\"\"\n",
        "        callbacks_list = [\n",
        "            # Save best model\n",
        "            callbacks.ModelCheckpoint(\n",
        "                model_path,\n",
        "                monitor='val_accuracy',\n",
        "                save_best_only=True,\n",
        "                save_weights_only=False,\n",
        "                verbose=1\n",
        "            ),\n",
        "\n",
        "            # Reduce learning rate on plateau\n",
        "            callbacks.ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=5,\n",
        "                min_lr=1e-7,\n",
        "                verbose=1\n",
        "            ),\n",
        "\n",
        "            # Early stopping\n",
        "            callbacks.EarlyStopping(\n",
        "                monitor='val_accuracy',\n",
        "                patience=10,\n",
        "                restore_best_weights=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "\n",
        "            # Learning rate scheduler\n",
        "            callbacks.LearningRateScheduler(\n",
        "                lambda epoch: 0.001 * (0.95 ** epoch)\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        return callbacks_list\n",
        "\n",
        "    def train(self, train_data, train_labels, epochs=50, validation_split=0.2):\n",
        "        \"\"\"\n",
        "        Train the model with optimized settings\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            self.create_efficient_model()\n",
        "\n",
        "        print(\"Creating data generators...\")\n",
        "        train_gen, val_gen = self.create_data_generators(\n",
        "            train_data, train_labels, validation_split\n",
        "        )\n",
        "\n",
        "        print(\"Setting up callbacks...\")\n",
        "        callback_list = self.get_callbacks()\n",
        "\n",
        "        print(\"Starting training...\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        self.history = self.model.fit(\n",
        "            train_gen,\n",
        "            validation_data=val_gen,\n",
        "            epochs=epochs,\n",
        "            callbacks=callback_list,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        training_time = time.time() - start_time\n",
        "        print(f\"Training completed in {training_time:.2f} seconds\")\n",
        "\n",
        "        return self.history\n",
        "\n",
        "    def evaluate_model(self, test_data, test_labels):\n",
        "        \"\"\"\n",
        "        Comprehensive model evaluation\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not trained yet!\")\n",
        "\n",
        "        print(\"Evaluating model...\")\n",
        "\n",
        "        # Basic evaluation\n",
        "        test_loss, test_accuracy = self.model.evaluate(\n",
        "            test_data, test_labels, verbose=0\n",
        "        )\n",
        "\n",
        "        # Predictions\n",
        "        predictions = self.model.predict(test_data, verbose=0)\n",
        "        predicted_classes = np.argmax(predictions, axis=1)\n",
        "\n",
        "        # Classification report\n",
        "        class_names = [str(i) for i in range(10)]\n",
        "        report = classification_report(\n",
        "            test_labels, predicted_classes,\n",
        "            target_names=class_names,\n",
        "            output_dict=True\n",
        "        )\n",
        "\n",
        "        # Confusion matrix\n",
        "        cm = confusion_matrix(test_labels, predicted_classes)\n",
        "\n",
        "        results = {\n",
        "            'test_accuracy': test_accuracy,\n",
        "            'test_loss': test_loss,\n",
        "            'classification_report': report,\n",
        "            'confusion_matrix': cm,\n",
        "            'predictions': predictions,\n",
        "            'predicted_classes': predicted_classes\n",
        "        }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def plot_training_history(self):\n",
        "        \"\"\"\n",
        "        Plot training history\n",
        "        \"\"\"\n",
        "        if self.history is None:\n",
        "            print(\"No training history available!\")\n",
        "            return\n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "        # Accuracy plot\n",
        "        ax1.plot(self.history.history['accuracy'], label='Training Accuracy')\n",
        "        ax1.plot(self.history.history['val_accuracy'], label='Validation Accuracy')\n",
        "        ax1.set_title('Model Accuracy')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.set_ylabel('Accuracy')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True)\n",
        "\n",
        "        # Loss plot\n",
        "        ax2.plot(self.history.history['loss'], label='Training Loss')\n",
        "        ax2.plot(self.history.history['val_loss'], label='Validation Loss')\n",
        "        ax2.set_title('Model Loss')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.set_ylabel('Loss')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def plot_confusion_matrix(self, cm):\n",
        "        \"\"\"\n",
        "        Plot confusion matrix\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(\n",
        "            cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=range(10), yticklabels=range(10)\n",
        "        )\n",
        "        plt.title('Confusion Matrix - Bangla Digit Recognition')\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.show()\n",
        "\n",
        "    def plot_sample_predictions(self, test_data, test_labels, num_samples=10):\n",
        "        \"\"\"\n",
        "        Plot sample predictions\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not trained yet!\")\n",
        "\n",
        "        # Get random samples\n",
        "        indices = np.random.choice(len(test_data), num_samples, replace=False)\n",
        "        sample_images = test_data[indices]\n",
        "        sample_labels = test_labels[indices]\n",
        "\n",
        "        # Make predictions\n",
        "        predictions = self.model.predict(sample_images, verbose=0)\n",
        "        predicted_classes = np.argmax(predictions, axis=1)\n",
        "        confidences = np.max(predictions, axis=1)\n",
        "\n",
        "        # Plot\n",
        "        fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "        axes = axes.ravel()\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            axes[i].imshow(sample_images[i].squeeze(), cmap='gray')\n",
        "            axes[i].set_title(\n",
        "                f'True: {sample_labels[i]}, Pred: {predicted_classes[i]}\\n'\n",
        "                f'Confidence: {confidences[i]:.3f}'\n",
        "            )\n",
        "            axes[i].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def predict_single_image(self, image_path_or_array):\n",
        "        \"\"\"\n",
        "        Predict a single image\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not trained yet!\")\n",
        "\n",
        "        if isinstance(image_path_or_array, str):\n",
        "            # Load image from path\n",
        "            image = cv2.imread(image_path_or_array, cv2.IMREAD_GRAYSCALE)\n",
        "            if image is None:\n",
        "                raise ValueError(f\"Could not load image from {image_path_or_array}\")\n",
        "        else:\n",
        "            image = image_path_or_array\n",
        "\n",
        "        # Preprocess image\n",
        "        image = cv2.resize(image, (32, 32))\n",
        "        image = image.astype('float32') / 255.0\n",
        "        image = np.expand_dims(image, axis=[0, -1])\n",
        "\n",
        "        # Predict\n",
        "        prediction = self.model.predict(image, verbose=0)\n",
        "        predicted_class = np.argmax(prediction)\n",
        "        confidence = np.max(prediction)\n",
        "\n",
        "        return predicted_class, confidence\n",
        "\n",
        "    def save_model(self, filepath='bangla_digit_model.h5'):\n",
        "        \"\"\"\n",
        "        Save the trained model\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"No model to save!\")\n",
        "\n",
        "        self.model.save(filepath)\n",
        "        print(f\"Model saved to {filepath}\")\n",
        "\n",
        "    def load_model(self, filepath='bangla_digit_model.h5'):\n",
        "        \"\"\"\n",
        "        Load a pre-trained model\n",
        "        \"\"\"\n",
        "        self.model = keras.models.load_model(filepath)\n",
        "        print(f\"Model loaded from {filepath}\")\n",
        "\n",
        "def load_bhand_dataset(data_path):\n",
        "\n",
        "    print(\"Loading BHAND dataset from pickled file...\")\n",
        "\n",
        "    try:\n",
        "        # Load the pickled dataset\n",
        "        with gzip.open(data_path, 'rb') as f:\n",
        "            train_set, valid_set, test_set = pickle.load(f,encoding = 'latin1')\n",
        "\n",
        "\n",
        "        # Extract data and labels\n",
        "        train_data, train_labels = train_set\n",
        "        val_data, val_labels = valid_set\n",
        "        test_data, test_labels = test_set\n",
        "\n",
        "        # Convert to numpy arrays if they aren't already\n",
        "        train_data = np.array(train_data, dtype='float32')\n",
        "        train_labels = np.array(train_labels, dtype='int32')\n",
        "        val_data = np.array(val_data, dtype='float32')\n",
        "        val_labels = np.array(val_labels, dtype='int32')\n",
        "        test_data = np.array(test_data, dtype='float32')\n",
        "        test_labels = np.array(test_labels, dtype='int32')\n",
        "\n",
        "        # Check if normalization is needed (values should be 0-1)\n",
        "        if train_data.max() > 1.5:  # Likely 0-255 range\n",
        "            print(\"Normalizing pixel values from 0-255 to 0-1...\")\n",
        "            train_data = train_data / 255.0\n",
        "            val_data = val_data / 255.0\n",
        "            test_data = test_data / 255.0\n",
        "\n",
        "        # Determine image dimensions and reshape if necessary\n",
        "        # BHAND images are typically 32x32 or flattened 1024-dimensional vectors\n",
        "        if len(train_data.shape) == 2:  # Flattened images (N, 1024)\n",
        "            img_size = int(np.sqrt(train_data.shape[1]))  # Should be 32 for 1024 features\n",
        "            print(f\"Reshaping flattened images to {img_size}x{img_size}...\")\n",
        "\n",
        "            train_data = train_data.reshape(-1, img_size, img_size, 1)\n",
        "            val_data = val_data.reshape(-1, img_size, img_size, 1)\n",
        "            test_data = test_data.reshape(-1, img_size, img_size, 1)\n",
        "        elif len(train_data.shape) == 3:  # Images without channel dimension (N, H, W)\n",
        "            print(\"Adding channel dimension...\")\n",
        "            train_data = np.expand_dims(train_data, axis=-1)\n",
        "            val_data = np.expand_dims(val_data, axis=-1)\n",
        "            test_data = np.expand_dims(test_data, axis=-1)\n",
        "\n",
        "        print(f\"Training data shape: {train_data.shape}\")\n",
        "        print(f\"Validation data shape: {val_data.shape}\")\n",
        "        print(f\"Test data shape: {test_data.shape}\")\n",
        "        print(f\"Number of classes: {len(np.unique(train_labels))}\")\n",
        "        print(f\"Label range: {train_labels.min()} to {train_labels.max()}\")\n",
        "\n",
        "        # Combine train and validation sets for training (since the model will split internally)\n",
        "        combined_train_data = np.concatenate([train_data, val_data], axis=0)\n",
        "        combined_train_labels = np.concatenate([train_labels, val_labels], axis=0)\n",
        "\n",
        "        return combined_train_data, combined_train_labels, test_data, test_labels\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âŒ Dataset file not found at: {data_path}\")\n",
        "        print(\"Please ensure 'bhand.pkl.gz' is in the specified path\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error loading dataset: {e}\")\n",
        "        print(\"Please check if the file format is correct\")\n",
        "        raise\n",
        "\n",
        "def create_sample_images(test_data, test_labels, num_samples=20):\n",
        "    \"\"\"\n",
        "    Create sample images for visualization\n",
        "    \"\"\"\n",
        "    indices = np.random.choice(len(test_data), num_samples, replace=False)\n",
        "    sample_images = test_data[indices]\n",
        "    sample_labels = test_labels[indices]\n",
        "\n",
        "    fig, axes = plt.subplots(4, 5, figsize=(12, 10))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        axes[i].imshow(sample_images[i].squeeze(), cmap='gray')\n",
        "        axes[i].set_title(f'Digit: {sample_labels[i]}')\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.suptitle('Sample Bangla Digits from Dataset')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def analyze_dataset(train_data, train_labels, test_data, test_labels):\n",
        "    \"\"\"\n",
        "    Analyze and visualize dataset statistics\n",
        "    \"\"\"\n",
        "    print(\"\\nðŸ“Š Dataset Analysis:\")\n",
        "    print(\"=\" * 40)\n",
        "\n",
        "    # Class distribution\n",
        "    unique_labels, train_counts = np.unique(train_labels, return_counts=True)\n",
        "    unique_labels_test, test_counts = np.unique(test_labels, return_counts=True)\n",
        "\n",
        "    # Create distribution plot\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Training set distribution\n",
        "    ax1.bar(unique_labels, train_counts, alpha=0.7, color='skyblue')\n",
        "    ax1.set_title('Training Set Class Distribution')\n",
        "    ax1.set_xlabel('Digit Class')\n",
        "    ax1.set_ylabel('Number of Samples')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Test set distribution\n",
        "    ax2.bar(unique_labels_test, test_counts, alpha=0.7, color='lightcoral')\n",
        "    ax2.set_title('Test Set Class Distribution')\n",
        "    ax2.set_xlabel('Digit Class')\n",
        "    ax2.set_ylabel('Number of Samples')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print statistics\n",
        "    print(f\"Total training samples: {len(train_data):,}\")\n",
        "    print(f\"Total test samples: {len(test_data):,}\")\n",
        "    print(f\"Image shape: {train_data.shape[1:]}\")\n",
        "    print(f\"Number of classes: {len(unique_labels)}\")\n",
        "\n",
        "    print(\"\\nClass distribution (Training):\")\n",
        "    for label, count in zip(unique_labels, train_counts):\n",
        "        percentage = (count / len(train_labels)) * 100\n",
        "        print(f\"  Digit {label}: {count:,} samples ({percentage:.1f}%)\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main training and evaluation pipeline\n",
        "    \"\"\"\n",
        "    print(\"ðŸ”¥ Efficient Bangla Digit Recognition System ðŸ”¥\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Configuration\n",
        "    DATASET_PATH = \"/content/drive/MyDrive/CSE463/Project/bhand.pkl.gz\"  # Path to the pickled dataset file\n",
        "    MODEL_SAVE_PATH = \"bangla_digit_model_efficient.h5\"\n",
        "    EPOCHS = 50\n",
        "\n",
        "    try:\n",
        "        # Load dataset\n",
        "        train_data, train_labels, test_data, test_labels = load_bhand_dataset(DATASET_PATH)\n",
        "\n",
        "        # Analyze dataset\n",
        "        analyze_dataset(train_data, train_labels, test_data, test_labels)\n",
        "\n",
        "        # Show sample images\n",
        "        print(\"\\nðŸ–¼ï¸  Displaying sample images...\")\n",
        "        create_sample_images(test_data, test_labels)\n",
        "\n",
        "        # Create and train model\n",
        "        recognizer = BanglaDigitRecognizer()\n",
        "\n",
        "        print(\"\\nðŸ“Š Model Architecture:\")\n",
        "        model = recognizer.create_efficient_model()\n",
        "        model.summary()\n",
        "\n",
        "        print(f\"\\nðŸš€ Training model for {EPOCHS} epochs...\")\n",
        "        history = recognizer.train(\n",
        "            train_data, train_labels,\n",
        "            epochs=EPOCHS,\n",
        "            validation_split=0.2\n",
        "        )\n",
        "\n",
        "        # Evaluate model\n",
        "        print(\"\\nðŸ“ˆ Evaluating model...\")\n",
        "        results = recognizer.evaluate_model(test_data, test_labels)\n",
        "\n",
        "        print(f\"\\nðŸŽ¯ Test Accuracy: {results['test_accuracy']:.4f}\")\n",
        "        print(f\"ðŸ“‰ Test Loss: {results['test_loss']:.4f}\")\n",
        "\n",
        "        # Plot results\n",
        "        print(\"\\nðŸ“Š Plotting training history...\")\n",
        "        recognizer.plot_training_history()\n",
        "\n",
        "        print(\"\\nðŸ” Plotting confusion matrix...\")\n",
        "        recognizer.plot_confusion_matrix(results['confusion_matrix'])\n",
        "\n",
        "        print(\"\\nðŸ–¼ï¸  Sample predictions...\")\n",
        "        recognizer.plot_sample_predictions(test_data, test_labels)\n",
        "\n",
        "        # Save model\n",
        "        recognizer.save_model(MODEL_SAVE_PATH)\n",
        "\n",
        "        print(\"\\nâœ… Training completed successfully!\")\n",
        "        print(f\"Model saved as: {MODEL_SAVE_PATH}\")\n",
        "\n",
        "        # Show detailed classification report\n",
        "        print(\"\\nðŸ“‹ Detailed Classification Report:\")\n",
        "        print(\"-\" * 50)\n",
        "        for class_name, metrics in results['classification_report'].items():\n",
        "            if isinstance(metrics, dict) and class_name.isdigit():\n",
        "                print(f\"Digit {class_name}:\")\n",
        "                print(f\"  Precision: {metrics['precision']:.4f}\")\n",
        "                print(f\"  Recall:    {metrics['recall']:.4f}\")\n",
        "                print(f\"  F1-Score:  {metrics['f1-score']:.4f}\")\n",
        "                print()\n",
        "\n",
        "        # Overall metrics\n",
        "        if 'accuracy' in results['classification_report']:\n",
        "            overall_acc = results['classification_report']['accuracy']\n",
        "            print(f\"Overall Accuracy: {overall_acc:.4f}\")\n",
        "\n",
        "        if 'macro avg' in results['classification_report']:\n",
        "            macro_avg = results['classification_report']['macro avg']\n",
        "            print(f\"Macro Average F1: {macro_avg['f1-score']:.4f}\")\n",
        "\n",
        "        # Performance summary\n",
        "        print(\"\\nðŸ“Š Performance Summary:\")\n",
        "        print(\"-\" * 30)\n",
        "        print(f\"ðŸŽ¯ Test Accuracy: {results['test_accuracy']:.2%}\")\n",
        "        print(f\"ðŸ“‰ Test Loss: {results['test_loss']:.4f}\")\n",
        "        print(f\"ðŸ”¢ Total Parameters: {model.count_params():,}\")\n",
        "\n",
        "        # Create inference example\n",
        "        print(\"\\nðŸš€ Testing single image prediction...\")\n",
        "        # Use a random test image for demonstration\n",
        "        random_idx = np.random.randint(0, len(test_data))\n",
        "        test_image = test_data[random_idx]\n",
        "        true_label = test_labels[random_idx]\n",
        "\n",
        "        predicted_digit, confidence = recognizer.predict_single_image(test_image)\n",
        "        print(f\"True digit: {true_label}\")\n",
        "        print(f\"Predicted digit: {predicted_digit}\")\n",
        "        print(f\"Confidence: {confidence:.3f}\")\n",
        "        print(f\"Correct: {'âœ…' if predicted_digit == true_label else 'âŒ'}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error: {e}\")\n",
        "        print(\"Please ensure the BHAND dataset file 'bhand.pkl.gz' is in the current directory.\")\n",
        "        print(\"Dataset should be available at: https://github.com/SaadatChowdhury/BHaND\")\n",
        "        print(\"Make sure to use Python 2/3 compatible pickle loading if needed.\")\n",
        "\n",
        "# Additional utility functions\n",
        "def benchmark_model(recognizer, test_data, num_samples=1000):\n",
        "    \"\"\"\n",
        "    Benchmark model inference speed\n",
        "    \"\"\"\n",
        "    print(f\"\\nâš¡ Benchmarking inference speed with {num_samples} samples...\")\n",
        "\n",
        "    # Select random samples\n",
        "    indices = np.random.choice(len(test_data), min(num_samples, len(test_data)), replace=False)\n",
        "    benchmark_data = test_data[indices]\n",
        "\n",
        "    # Time the inference\n",
        "    start_time = time.time()\n",
        "    predictions = recognizer.model.predict(benchmark_data, verbose=0)\n",
        "    end_time = time.time()\n",
        "\n",
        "    total_time = end_time - start_time\n",
        "    avg_time = total_time / len(benchmark_data)\n",
        "    fps = len(benchmark_data) / total_time\n",
        "\n",
        "    print(f\"Total inference time: {total_time:.3f} seconds\")\n",
        "    print(f\"Average time per image: {avg_time*1000:.2f} ms\")\n",
        "    print(f\"Inference speed: {fps:.1f} FPS\")\n",
        "\n",
        "def export_model_info(recognizer, filepath='model_info.txt'):\n",
        "    \"\"\"\n",
        "    Export model information to text file\n",
        "    \"\"\"\n",
        "    if recognizer.model is None:\n",
        "        print(\"No model to export info for!\")\n",
        "        return\n",
        "\n",
        "    with open(filepath, 'w') as f:\n",
        "        f.write(\"Bangla Digit Recognition Model Information\\n\")\n",
        "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "        f.write(f\"Model Architecture: CNN with Batch Normalization\\n\")\n",
        "        f.write(f\"Input Shape: {recognizer.input_shape}\\n\")\n",
        "        f.write(f\"Number of Classes: {recognizer.num_classes}\\n\")\n",
        "        f.write(f\"Total Parameters: {recognizer.model.count_params():,}\\n\")\n",
        "        f.write(f\"Model Size: {os.path.getsize('bangla_digit_model_efficient.h5') / (1024*1024):.2f} MB\\n\")\n",
        "\n",
        "        if recognizer.history:\n",
        "            final_acc = recognizer.history.history['accuracy'][-1]\n",
        "            final_val_acc = recognizer.history.history['val_accuracy'][-1]\n",
        "            f.write(f\"Final Training Accuracy: {final_acc:.4f}\\n\")\n",
        "            f.write(f\"Final Validation Accuracy: {final_val_acc:.4f}\\n\")\n",
        "\n",
        "    print(f\"Model information exported to {filepath}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqX1D7X_YBmE"
      },
      "source": [
        "# **Percentage-Point Gap (pp)**\n",
        "\n",
        "percentage-point gap between the final training accuracy and the final validation (or test) accuracy:\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "*Practical Rule of Thumb:*\n",
        "> **|Î” pp| < 0.3 â†’ model generalises perfectly, indicating no measurable over-fitting.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5rOsXgi6LslX"
      },
      "outputs": [],
      "source": [
        "# 1. reload the trained model\n",
        "rec = BanglaDigitRecognizer()\n",
        "rec.load_model(\"bangla_digit_model_efficient.h5\")\n",
        "\n",
        "# 2. reload the dataset\n",
        "train_x, train_y, _, _ = load_bhand_dataset(\"/content/drive/MyDrive/CSE463/Project/bhand.pkl.gz\")\n",
        "\n",
        "# 3. recreate the 80/20 split that ImageDataGenerator used\n",
        "split = int(len(train_x) * 0.8)\n",
        "x_train_part, y_train_part = train_x[:split], train_y[:split]\n",
        "x_val_part,   y_val_part   = train_x[split:], train_y[split:]\n",
        "\n",
        "# 4. evaluate\n",
        "train_acc = rec.model.evaluate(x_train_part, y_train_part, verbose=0)[1]\n",
        "val_acc   = rec.model.evaluate(x_val_part,   y_val_part,   verbose=0)[1]\n",
        "delta_pp  = (train_acc - val_acc) * 100\n",
        "print(f\"Î” pp (train â€“ val) = {delta_pp:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4pBmyQmk7dH"
      },
      "source": [
        "# **BHAND comparisons**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ILw0E6lVAMlC"
      },
      "outputs": [],
      "source": [
        "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "# â•‘        BHAND BENCHMARK  â€“  EfficientCNN + AlexNet + LeNet + MBV2 â•‘\n",
        "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "import os, time, pathlib, warnings, numpy as np, pandas as pd, tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "tf.random.set_seed(42); np.random.seed(42)\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 0  Dataset + EfficientCNN\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "DATASET_PATH = \"/content/drive/MyDrive/CSE463/Project/bhand.pkl.gz\"\n",
        "EFF_H5       = pathlib.Path(\"bangla_digit_model_efficient.h5\")\n",
        "CKPT_DIR     = pathlib.Path(\"checkpoints\"); CKPT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "train_x, train_y, test_x, test_y = load_bhand_dataset(DATASET_PATH)\n",
        "\n",
        "rec = BanglaDigitRecognizer(); rec.create_efficient_model()\n",
        "rec.load_model(EFF_H5)\n",
        "# (compile once so loss/metric objects exist)\n",
        "rec.model.compile(\"adam\",\"sparse_categorical_crossentropy\",\n",
        "                  metrics=[\"sparse_categorical_accuracy\"])\n",
        "eff_model = rec.model\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 1  Baseline builders\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def build_lenet():\n",
        "    m = keras.Sequential(name=\"LeNet5\")\n",
        "    m.add(layers.Conv2D(6,5,padding=\"same\",activation=\"relu\",input_shape=(32,32,1)))\n",
        "    m.add(layers.AveragePooling2D(2)); m.add(layers.Conv2D(16,5,activation=\"relu\"))\n",
        "    m.add(layers.AveragePooling2D(2)); m.add(layers.Flatten())\n",
        "    m.add(layers.Dense(120,activation=\"relu\"))\n",
        "    m.add(layers.Dense(84,activation=\"relu\"))\n",
        "    m.add(layers.Dense(10,activation=\"softmax\"))\n",
        "    return m\n",
        "\n",
        "def build_alexnet():\n",
        "    m = keras.Sequential(name=\"AlexNetBHAND\")\n",
        "    m.add(layers.Conv2D(96,3,padding=\"same\",activation=\"relu\",input_shape=(32,32,1)))\n",
        "    m.add(layers.BatchNormalization()); m.add(layers.MaxPooling2D(2))\n",
        "    m.add(layers.Conv2D(256,3,padding=\"same\",activation=\"relu\"))\n",
        "    m.add(layers.BatchNormalization()); m.add(layers.MaxPooling2D(2))\n",
        "    m.add(layers.Conv2D(384,3,padding=\"same\",activation=\"relu\"))\n",
        "    m.add(layers.Conv2D(384,3,padding=\"same\",activation=\"relu\"))\n",
        "    m.add(layers.Conv2D(256,3,padding=\"same\",activation=\"relu\"))\n",
        "    m.add(layers.MaxPooling2D(2)); m.add(layers.Flatten())\n",
        "    m.add(layers.Dense(512,activation=\"relu\")); m.add(layers.Dropout(0.5))\n",
        "    m.add(layers.Dense(512,activation=\"relu\")); m.add(layers.Dropout(0.5))\n",
        "    m.add(layers.Dense(10,activation=\"softmax\"))\n",
        "    return m\n",
        "\n",
        "def build_mobilenet(alpha=0.5):\n",
        "    base = tf.keras.applications.MobileNetV2(\n",
        "        input_shape=(32,32,3), alpha=alpha, include_top=False, weights=None)\n",
        "    m = keras.Sequential(name=f\"MobileNetV2_{alpha}\")\n",
        "    m.add(layers.Input((32,32,1)))\n",
        "    m.add(layers.Conv2D(3,(1,1),padding=\"same\"))\n",
        "    m.add(base); m.add(layers.GlobalAveragePooling2D())\n",
        "    m.add(layers.Dense(10,activation=\"softmax\"))\n",
        "    return m\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 2  Safe trainer / loader\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def prep(model, x, y, epochs, h5):\n",
        "    x = x.astype(\"float32\"); y = y.astype(\"int32\")\n",
        "    if pathlib.Path(h5).exists():\n",
        "        model.load_weights(h5); print(f\"âœ“ Loaded {pathlib.Path(h5).name}\")\n",
        "    else:\n",
        "        model.compile(\"adam\",\"sparse_categorical_crossentropy\",\n",
        "                      metrics=[\"sparse_categorical_accuracy\"])\n",
        "        model.fit(x, y, epochs=epochs, batch_size=64,\n",
        "                  validation_split=0.1, verbose=2)\n",
        "        model.save(h5)\n",
        "    return model\n",
        "\n",
        "alex_model   = prep(build_alexnet(),      train_x, train_y, 40, CKPT_DIR/\"AlexNet.h5\")\n",
        "lenet_model  = prep(build_lenet(),        train_x, train_y, 20, CKPT_DIR/\"LeNet5.h5\")\n",
        "mobile_model = prep(build_mobilenet(0.5), train_x, train_y, 30, CKPT_DIR/\"MobileNetV2.h5\")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 3  Evaluation helper (auto-compile if needed)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def evaluate(model, x, y, h5):\n",
        "    model.compile(                       # compile every time (cheap)\n",
        "        optimizer=\"adam\",\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"sparse_categorical_accuracy\"]\n",
        "    )\n",
        "    acc = model.evaluate(x, y, verbose=0)[1]\n",
        "    t0 = time.time(); model.predict(x[:1024], verbose=0); lat = (time.time()-t0)/1024*1000\n",
        "    size = pathlib.Path(h5).stat().st_size/1_048_576\n",
        "    return acc, model.count_params(), lat, size\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 4  FLOP counter  â€“ returns None if profiling not available\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def safe_flops(model):\n",
        "    try:\n",
        "        g = tf.function(lambda x: model(x)).get_concrete_function(\n",
        "                tf.TensorSpec([1,32,32,1], tf.float32)).graph\n",
        "        return tf.profiler.experimental.get_stats(g)['total_flops']/1e6\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# 5  Collect metrics\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "rows = {}\n",
        "for name, mdl, h5 in [\n",
        "        (\"EfficientCNN\",   eff_model,   EFF_H5),\n",
        "        (\"AlexNet\",        alex_model,  CKPT_DIR/\"AlexNet.h5\"),\n",
        "        (\"LeNet5\",         lenet_model, CKPT_DIR/\"LeNet5.h5\"),\n",
        "        (\"MobileNetV2-0.5\",mobile_model,CKPT_DIR/\"MobileNetV2.h5\")]:\n",
        "    acc, params, lat, size = evaluate(mdl, test_x, test_y, h5)\n",
        "    flop_val = safe_flops(mdl)\n",
        "    rows[name] = dict(\n",
        "        accuracy = round(acc,4),\n",
        "        params   = params,\n",
        "        FLOPs_M  = \"-\" if flop_val is None else round(flop_val,1),\n",
        "        lat_ms   = round(lat,3),\n",
        "        size_mb  = round(size,2)\n",
        "    )\n",
        "\n",
        "df = pd.DataFrame(rows).T\n",
        "df.to_csv(\"benchmark_with_flops.csv\")\n",
        "\n",
        "print(\"\\n=== BHAND Benchmark ===\")\n",
        "print(df.to_string())\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
